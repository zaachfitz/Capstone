{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, os, time, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/E-Commerce Reviews Cleaned.csv', \n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 header=None,\n",
    "                 names=[\"Classification\", \"Review\"]) \n",
    "#can decode unicode characters without error, array each string is each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23486, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classification                                             Review\n",
       "0               1  Absolutely wonderful - silky and sexy and comf...\n",
       "1               1  Love this dress!  it's sooo pretty.  i happene...\n",
       "2               0  I had such high hopes for this dress and reall...\n",
       "3               1  I love, love, love this jumpsuit. it's fun, fl...\n",
       "4               1  This shirt is very flattering to all due to th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification      0\n",
       "Review            845\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification    0\n",
       "Review            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22641 entries, 0 to 23485\n",
      "Data columns (total 2 columns):\n",
      "Classification    22641 non-null int64\n",
      "Review            22641 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 530.6+ KB\n"
     ]
    }
   ],
   "source": [
    "m_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18540\n",
       "0     4101\n",
       "Name: Classification, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df.Classification.value_counts() #Overwhelming majority of reviews are classified as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(s): #make each word into their own vector of size n\n",
    "    s = unicode_to_ascii(s.lower().strip()) #all lowercase and then remove the spaces. Make sure you do this\n",
    "    #we take all the punctuation and separate it. it will mess up the system without it, like hello, is different then hello\n",
    "    s = re.sub(r\"([?.!;,:()\\\"])\", r\" \\1 \", s) #take first argument it matches and puts a space after it. inside [] is what we want to match\n",
    "    \n",
    "    s = re.sub(r'[\" \"]+', \" \", s) #replace spaces if you want to get rid of tabs it's \\t. thsi replaces with single space.\n",
    "    \n",
    "    s = re.sub(r\"[^a-zA-Z?.!;:,()\\\"]+\", \" \", s) #just removes anything that we do not want. exluding what is in []\n",
    "    \n",
    "    s = s.rstrip().strip()  #just remove anything at the start that we dont want.\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>' #Differentiate between different lengths of the arrays. Tokenization: all sentences are equal length, the biggest one acts as the size everyone else needs to be. so we add 0's. We do not want to consider anything after the end, aka the zeros.\n",
    "    \n",
    "    return s\n",
    "\n",
    "def tokenize(lang): #Actually doing the tokenizing. lang is entire dataset.\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') #uses keras, high level API that allows you to do functions without writing code\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang) #implementing it to read the dataset. Learn what longest one is ect.\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang) #takes each word in the sentence and turns it into a number that represents it.\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                          padding=\"post\") #adds the zeros we needed before. post is after pre is before.\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(dataframe): #actually process dataset\n",
    "    clean_reviews = [preprocess_sentence(s) for s in dataframe[\"Review\"]] #looks at all text in the review column and preprocess them all.\n",
    "    \n",
    "    input_tensor, input_tokenizer = tokenize(clean_reviews) #tokenize the reviews. same as tensor and lang_tokenizer. \n",
    "    target_tensor = dataframe[\"Classification\"].tolist()\n",
    "    \n",
    "    _, indices = np.unique(target_tensor, return_inverse=True) #_ means we don't care about some variables so we dont list them. grab everything that is different and make it into a number.\n",
    "    indices = indices.astype(np.int32) #makes our dtypes the same\n",
    "    target_tensor = np.expand_dims(indices, axis=1) #add dimension to columns (axis 1)\n",
    "    \n",
    "    return input_tensor, target_tensor, input_tokenizer\n",
    "\n",
    "#Dtypes for tensors need to match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, input_tokenizer = load_dataset(m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11, 256, 521, ...,   0,   0,   0],\n",
       "       [ 11,  27,   9, ...,   0,   0,   0],\n",
       "       [ 11,   3,  74, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 11,   9,  33, ...,   0,   0,   0],\n",
       "       [ 11,   3,  79, ...,   0,   0,   0],\n",
       "       [ 11,   9,  23, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_tensor) #words are numbers with our padded zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(target_tensor) #unicoded, we cant feed the labels into the system obv. We can fix in our load_dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22641, 137)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(input_tensor).shape #largest review is 137 so all arrays are 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22641, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(target_tensor).shape #This shows that we have 22,641 classified reviews, which is the same as our input_tensor\n",
    "#This also means that we were able to unicode all of the classifications successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential #sequential means any layer comes after eachother. \n",
    "#One after another. CANT USE SEQUENTIAL FOR RESIDUAL NETWORKS, thats dynamic\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Embedding \n",
    "#Embedding is first turns each word into vector.\n",
    "\n",
    "EMBEDDING_DIM = 2 #any power of two the higher is more computing power #how many numbers represent one word\n",
    "VOCAB_SIZE = len(input_tokenizer.word_index)+1 #this is each word. need to add one. Essentally how many words there are. Dict.\n",
    "INPUT_LENGTH = input_tensor.shape[1] #the second value from shape was word length. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #defining a blank model\n",
    "model.add(Embedding(VOCAB_SIZE,\n",
    "                    EMBEDDING_DIM,\n",
    "                    input_length=INPUT_LENGTH,\n",
    "                   name='embedding')) #translate text into vectors, adds a name to see different layers.\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(1, activation='sigmoid')) #the 1 is how many end points. sigmoid = closer to 0 it negative, 1 is positive\n",
    "\n",
    "#you can make it more complex if you add parameters to dense, activation can be relu and a differet number.\n",
    "\n",
    "#you have so many vectors and you need to flatten it for the system. Takes vectors and stacks them then adds.\n",
    "#instead of 137 vectors you have just one. i.e. (a,b so a+b=c) all vectors will end up being 60160 length (INPUT_LENGTH*EMBEDDING_DIM)\n",
    "#Each word has its own vector, so each vector needs to be stacked beside eachother. length is different than the vector. it's the size of the embedding\n",
    "#look up word2vec******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"]) #uses gradient descent. then binary 0 or 1 class. then shows accuracy\n",
    "\n",
    "\n",
    "model.fit(input_tensor, target_tensor, epochs=2, verbose=1) #verbose is seeing output, epochs is number of times through a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatically take a sentence and tokenize it then feeds it into the system. then do predict(\"This sucks\")\n",
    "\n",
    "def predict(sentence):\n",
    "    # creating cleaned input, output pairs\n",
    "    sentence_tokens = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence_tokens = input_tokenizer.texts_to_sequences([sentence_tokens])\n",
    "\n",
    "    sentence_length = input_tensor.shape[1]\n",
    "\n",
    "    for i, s in enumerate(sentence_tokens):\n",
    "        sentence_tokens[i] = s + ([0] * (sentence_length - len(s)))\n",
    "\n",
    "    result = model.predict(sentence_tokens)\n",
    "\n",
    "    if result[0][0] < 0.5:\n",
    "        print(\"'{0}' has a NEGATIVE sentiment with confidence {1}\".format(sentence, 1-result[0][0]))\n",
    "    elif result[0][0] >= 0.5:\n",
    "        print(\"'{0}' has a POSITIVE sentiment with confidence {1}\".format(sentence, result[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
